inherit_from: ../clotho_v2/waveform/train_data.yaml

experiment_path: experiments/dcase2022/cnn14rnn_2trm_scst

seed: 1

model:
    type: captioning.models.rl_model.ScstWrapper
    args: {}
    model:
        encoder:
            type: captioning.models.crnn_trm_encoder.Cnn14RnnEncoder
            args:
                freeze_cnn: True
                freeze_cnn_bn: True
            cnn:
                type: captioning.models.cnn_encoder.Cnn14Encoder
                args:
                    sample_rate: 32000
                pretrained: pretrained_feature_extractors/contrastive_pretrain_cnn14_bertm.pth
            rnn:
                type: captioning.models.rnn_encoder.RnnEncoder
                args:
                    bidirectional: True
                    hidden_size: 256
                    dropout: 0.5
                    num_layers: 3
                    spec_dim: -1
                    fc_feat_dim: 2048
                    attn_feat_dim: 2048
        decoder:
            type: captioning.models.transformer_decoder.TransformerDecoder
            args:
                vocab_size: 4368
                emb_dim: 256
                fc_emb_dim: 512
                attn_emb_dim: 512
                nlayers: 2
                dropout: 0.2
        type: captioning.models.transformer_model.TransformerModel
        args: {}
        pretrained: experiments/dcase2022/cnn14rnn_2trm/seed_1/swa.pth

specaug: False

optimizer:
    type: torch.optim.Adam
    args:
        lr: !!float 5e-5
        weight_decay: !!float 0.0

lr_scheduler:
    type: captioning.utils.lr_scheduler.ExponentialDecayScheduler
    args:
        warmup_iters: 0
        final_lrs: !!float 5e-5


trainer:
    max_grad_norm: 1.0
    epochs: 100
    save_interval: 10
    lr_update_interval: iteration
    monitor_metric:
        name: score
        mode: max
    include_optim_in_ckpt: False


inference_args:
    sample_method: beam
    beam_size: 3


scheduled_sampling:
    use: False
    mode: linear
    final_ratio: 0.7

loss:
    type: captioning.losses.loss.LabelSmoothingLoss
    args:
        smoothing: 0.1

swa:
    use: True
    start: 96